{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU7qK0iOEvIv"
      },
      "source": [
        "# Applying log and finding sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUx7OsnX-cFx",
        "outputId": "9c38a3f1-515c-41d6-8d55-abdb2df6af2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RRVR4N0JwU",
        "outputId": "a2aadd1c-8204-4277-a3b6-1b9f4e0c46d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Image Values for Girls:\n",
            "[149.61694678 170.20668093 139.1240222  147.83747453 149.54764257\n",
            " 157.69512856 151.92007673 148.76973092 147.32033783 142.23708514\n",
            " 139.83948587 147.89647367 144.75631039 148.81111679 139.37115801\n",
            " 144.73871817 155.33496462 154.21946372 150.90545656 144.2063519\n",
            " 139.13610782 146.5836082  155.09413068 109.22502164]\n",
            "Mean of Mean Image Values for Girls: 146.8497289254283\n",
            "\n",
            "Log Mean Image Values for Girls:\n",
            "[5.00807834 5.13701347 4.93536578 4.99611353 5.00761502 5.0606636\n",
            " 5.02335457 5.00239968 4.99260938 4.95749528 4.94049524 4.99651253\n",
            " 4.97505171 5.00267783 4.93714058 4.97493017 5.04558385 5.03837668\n",
            " 5.01665353 4.97124527 4.93545265 4.98759597 5.04403223 4.69341017]\n",
            "Mean of Log Mean Image Values for Girls: 4.986661126934616\n",
            "Standard Deviation of Log Standard Deviation Values for Girls: 0.3768977545046952\n",
            "\n",
            "Mean Image Values for Boys:\n",
            "[121.4385101  163.58040355 156.29971046 153.51149425 167.47992591\n",
            " 139.39419902 139.13174778 157.36128592 112.57254334 162.36813922\n",
            " 155.99122993 154.61821726 154.92916579 148.32974138 148.92054945\n",
            " 163.50229164 157.28265003 152.73395294 143.36209006 151.21656702\n",
            " 151.21087907 153.70492375 109.23368425 150.84789532 170.21047648]\n",
            "Mean of Mean Image Values for Boys: 149.56929095681232\n",
            "\n",
            "Log Mean Image Values for Boys:\n",
            "[4.79940804 5.09730463 5.05177538 5.03377545 5.1208635  4.93730588\n",
            " 4.93542131 5.05854435 4.72359784 5.08986622 5.04979979 5.04095896\n",
            " 5.04296802 4.99943778 5.00341294 5.09682701 5.05804451 5.02869754\n",
            " 4.96537353 5.01871303 5.01867541 5.03503468 4.69348948 5.01627201\n",
            " 5.13703577]\n",
            "Mean of Log Mean Image Values for Boys: 5.00210412250355\n",
            "Standard Deviation of Log Standard Deviation Values for Boys: 0.3081731064148334\n",
            "\n",
            "Pooled Standard Deviation of Log Standard Deviations: 25.24097969184794\n",
            "Sample Size for hypothesis testing: 204.29364922781133\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define the directory paths for girls and boys images\n",
        "girls_dir = '/content/drive/MyDrive/dataset/dataset/girls_data'\n",
        "boys_dir = '/content/drive/MyDrive/dataset/dataset/boys_data'\n",
        "\n",
        "# Function to calculate mean, log mean, and log standard deviation for a single image\n",
        "def calculate_log_stats(image_path):\n",
        "  image = cv2.imread(image_path)\n",
        "  pixels = image.flatten()\n",
        "  pixel_values = np.array(pixels)\n",
        "  mean = np.mean(pixel_values)\n",
        "  std_dev = np.std(pixel_values)\n",
        "  # Calculate log of mean and standard deviation\n",
        "  log_mean = np.log(mean)\n",
        "  log_std_dev = np.log(std_dev)\n",
        "  return mean, log_mean, log_std_dev\n",
        "\n",
        "# Function to process all images in a directory\n",
        "def process_images(directory):\n",
        "  image_means = []\n",
        "  log_means = []\n",
        "  log_std_devs = []\n",
        "  for file in os.listdir(directory):\n",
        "    if file.endswith(('jpg', 'jpeg', 'png')):\n",
        "      image_path = os.path.join(directory, file)\n",
        "      mean, log_mean, log_std_dev = calculate_log_stats(image_path)\n",
        "      image_means.append(mean)\n",
        "      log_means.append(log_mean)\n",
        "      log_std_devs.append(log_std_dev)\n",
        "  return np.array(image_means), np.array(log_means), np.array(log_std_devs)\n",
        "\n",
        "# Process images for girls and boys\n",
        "girls_means, girls_log_means, girls_log_std_devs = process_images(girls_dir)\n",
        "boys_means, boys_log_means, boys_log_std_devs = process_images(boys_dir)\n",
        "\n",
        "# Calculate statistics\n",
        "girls_mean_log_mean = np.mean(girls_log_means)\n",
        "girls_std_dev_log_std_dev = np.std(girls_log_std_devs)  # Standard deviation of log standard deviations for girls\n",
        "boys_mean_log_mean = np.mean(boys_log_means)\n",
        "boys_std_dev_log_std_dev = np.std(boys_log_std_devs)  # Standard deviation of log standard deviations for boys\n",
        "\n",
        "\n",
        "\n",
        "n1 = 24\n",
        "n2 = 25\n",
        "\n",
        "# Pooled standard deviation (assuming equal variances for simplicity)\n",
        "pooled_std_dev_log_std_dev =  (((n1)*girls_std_dev_log_std_dev**2 + (n2)*boys_std_dev_log_std_dev**2) /n1+n2)\n",
        "# Sample size (using the pooled standard deviation of log-transformed values)\n",
        "sample_size = 4 * pooled_std_dev_log_std_dev * ((1.96 + 1.282)**2) / ((np.exp(boys_mean_log_mean) - np.exp(girls_mean_log_mean))**2)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Image Values for Girls:\")\n",
        "print(girls_means)  # Print all mean values for girls' images\n",
        "print(\"Mean of Mean Image Values for Girls:\", np.mean(girls_means))\n",
        "\n",
        "print(\"\\nLog Mean Image Values for Girls:\")\n",
        "print(girls_log_means)  # Print all log mean values for girls' images\n",
        "print(\"Mean of Log Mean Image Values for Girls:\", girls_mean_log_mean)\n",
        "print(\"Standard Deviation of Log Standard Deviation Values for Girls:\", girls_std_dev_log_std_dev)\n",
        "\n",
        "print(\"\\nMean Image Values for Boys:\")\n",
        "print(boys_means)  # Print all mean values for boys' images\n",
        "print(\"Mean of Mean Image Values for Boys:\", np.mean(boys_means))\n",
        "\n",
        "print(\"\\nLog Mean Image Values for Boys:\")\n",
        "print(boys_log_means)  # Print all log mean values for boys' images\n",
        "print(\"Mean of Log Mean Image Values for Boys:\", boys_mean_log_mean)\n",
        "print(\"Standard Deviation of Log Standard Deviation Values for Boys:\", boys_std_dev_log_std_dev)\n",
        "\n",
        "print(\"\\nPooled Standard Deviation of Log Standard Deviations:\", pooled_std_dev_log_std_dev)\n",
        "print(\"Sample Size for hypothesis testing:\", sample_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYDr2Lqi4T6E"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omtSStOaERGh",
        "outputId": "a34e7992-3bfe-4bc6-859c-b3f5491291b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of loaded images: 49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image 1: Predicted skin type: oily\n",
            "Image 2: Predicted skin type: oily\n",
            "Image 3: Predicted skin type: dry\n",
            "Image 4: Predicted skin type: normal\n",
            "Image 5: Predicted skin type: normal\n",
            "Image 6: Predicted skin type: oily\n",
            "Image 7: Predicted skin type: oily\n",
            "Image 8: Predicted skin type: oily\n",
            "Image 9: Predicted skin type: oily\n",
            "Image 10: Predicted skin type: oily\n",
            "Image 11: Predicted skin type: oily\n",
            "Image 12: Predicted skin type: oily\n",
            "Image 13: Predicted skin type: oily\n",
            "Image 14: Predicted skin type: oily\n",
            "Image 15: Predicted skin type: oily\n",
            "Image 16: Predicted skin type: oily\n",
            "Image 17: Predicted skin type: oily\n",
            "Image 18: Predicted skin type: oily\n",
            "Image 19: Predicted skin type: dry\n",
            "Image 20: Predicted skin type: oily\n",
            "Image 21: Predicted skin type: oily\n",
            "Image 22: Predicted skin type: oily\n",
            "Image 23: Predicted skin type: oily\n",
            "Image 24: Predicted skin type: oily\n",
            "Image 25: Predicted skin type: oily\n",
            "Image 26: Predicted skin type: dry\n",
            "Image 27: Predicted skin type: oily\n",
            "Image 28: Predicted skin type: oily\n",
            "Image 29: Predicted skin type: oily\n",
            "Image 30: Predicted skin type: oily\n",
            "Image 31: Predicted skin type: oily\n",
            "Image 32: Predicted skin type: oily\n",
            "Image 33: Predicted skin type: normal\n",
            "Image 34: Predicted skin type: oily\n",
            "Image 35: Predicted skin type: dry\n",
            "Image 36: Predicted skin type: oily\n",
            "Image 37: Predicted skin type: oily\n",
            "Image 38: Predicted skin type: oily\n",
            "Image 39: Predicted skin type: oily\n",
            "Image 40: Predicted skin type: oily\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(folder_path):\n",
        "    image_paths = []\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
        "                    image_path = os.path.join(root, file)\n",
        "                    image_paths.append(image_path)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Folder not found. Please check the folder path.\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred while loading the data:\", e)\n",
        "    return image_paths\n",
        "\n",
        "def load_and_preprocess_images(image_paths):\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Error loading image: {image_path}\")\n",
        "            continue\n",
        "        # Preprocess image\n",
        "        # Resize images to a fixed size\n",
        "        resized_image = cv2.resize(image, (224, 224))\n",
        "\n",
        "        images.append(resized_image)\n",
        "    return images\n",
        "\n",
        "# Load images\n",
        "folder_path = '/content/drive/MyDrive/dataset/dataset/mini_dataset/'\n",
        "image_paths = load_data(folder_path)\n",
        "\n",
        "# Load and preprocess images\n",
        "X = load_and_preprocess_images(image_paths)\n",
        "\n",
        "# Print the number of loaded images\n",
        "print(\"Number of loaded images:\", len(X))\n",
        "\n",
        "# Split data into initially labeled and unlabeled sets\n",
        "X_labeled, X_unlabeled = train_test_split(X, test_size=0.8, random_state=42)\n",
        "\n",
        "# Clustering (Unsupervised Learning)\n",
        "n_clusters = 3  # Number of clusters (since you have 3 skin types)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_ids = kmeans.fit_predict(np.array(X_unlabeled).reshape(len(X_unlabeled), -1))\n",
        "cluster_labels = {0: 'oily', 1: 'dry', 2: 'normal'}\n",
        "\n",
        "# Assign labels to unlabelled data based on cluster assignments\n",
        "y_pred_unlabeled = [cluster_labels[cluster_id] for cluster_id in cluster_ids]\n",
        "\n",
        "# Print cluster assignments for review\n",
        "for i, label in enumerate(y_pred_unlabeled):\n",
        "    print(f\"Image {i + 1}: Predicted skin type: {label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the data"
      ],
      "metadata": {
        "id": "SaqyrCLZPLY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQr0EVIBAEaP",
        "outputId": "baa15c3f-cb98-4c7d-b333-2add95cf18a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 40\n",
            "Number of labels: 40\n",
            "First ten labels: ['oily', 'oily', 'oily', 'dry', 'oily', 'oily', 'oily', 'dry', 'oily', 'normal', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'dry', 'normal', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily', 'dry', 'normal', 'oily', 'oily', 'oily', 'oily', 'oily', 'oily']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def load_data(folder_path):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png', '.gif')):  # Add more image extensions if needed\n",
        "                    image_path = os.path.join(root, file)\n",
        "                    image_paths.append(image_path)\n",
        "                    labels.append(os.path.basename(os.path.dirname(image_path)))  # Use parent folder name as label\n",
        "    except FileNotFoundError:\n",
        "        print(\"Folder not found. Please check the folder path.\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred while loading the data:\", e)\n",
        "    return image_paths, labels\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error loading image: {image_path}\")\n",
        "        return None\n",
        "    resized_image = cv2.resize(image, (224, 224))  # Resize to 224x224\n",
        "    # You can add more preprocessing steps here\n",
        "    return resized_image\n",
        "\n",
        "# Load images and labels\n",
        "folder_path = '/content/drive/MyDrive/Dataset'\n",
        "image_paths, labels = load_data(folder_path)\n",
        "\n",
        "import random\n",
        "\n",
        "# Shuffle the data\n",
        "combined = list(zip(image_paths, labels))\n",
        "random.shuffle(combined)\n",
        "image_paths[:], labels[:] = zip(*combined)\n",
        "\n",
        "# Print loaded data\n",
        "print(\"Number of images:\", len(image_paths))\n",
        "print(\"Number of labels:\", len(labels))\n",
        "print(\"First ten labels:\", labels[:40])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_paths, X_test, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPOqH3SfceDo",
        "outputId": "c16ac3fd-991b-4462-efbb-dc36d662f099"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Dataset/normal/img 33.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 1.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 24.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 38.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 14.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 31.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 34.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 13.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/dry/img 26.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/dry/img 35.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/dry/img 3.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 9.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 28.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 40.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 7.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 21.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 16.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 11.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 17.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 10.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/normal/=img 4.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 32.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 22.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 37.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 2.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 23.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 36.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 18.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 8.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 6.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 39.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 15.jpeg']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VEdjQBhd8H4",
        "outputId": "5bab36d1-90b1-4ead-a132-9124f30b0806"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Dataset/oily/img 17.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 37.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 32.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 16.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 30.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/normal/img 33.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 18.jpeg',\n",
              " '/content/drive/MyDrive/Dataset/oily/img 28.jpeg']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "lsymwYgbPRbg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffSqSPnghQs1",
        "outputId": "3fede1ea-60cb-4bd7-f94e-320d20b678f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics for SVM with best parameters:\n",
            "Accuracy: 0.625\n",
            "Precision: 0.765625\n",
            "Recall: 0.625\n",
            "F1: 0.4807692307692308\n",
            "Metrics for SVM saved to SVM_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "\n",
        "# Define the paths to your data folders\n",
        "base_dir = '/content/drive/MyDrive/Dataset'\n",
        "train_oily_dir = os.path.join(base_dir, 'oily')\n",
        "train_dry_dir = os.path.join(base_dir, 'dry')\n",
        "train_normal_dir = os.path.join(base_dir, 'normal')\n",
        "\n",
        "# Collect paths to all images in each class folder\n",
        "oily_imgs = [os.path.join(train_oily_dir, img) for img in os.listdir(train_oily_dir)]\n",
        "dry_imgs = [os.path.join(train_dry_dir, img) for img in os.listdir(train_dry_dir)]\n",
        "normal_imgs = [os.path.join(train_normal_dir, img) for img in os.listdir(train_normal_dir)]\n",
        "\n",
        "# Concatenate all paths and assign labels\n",
        "X_paths = oily_imgs + dry_imgs + normal_imgs\n",
        "y = ['oily'] * len(oily_imgs) + ['dry'] * len(dry_imgs) + ['normal'] * len(normal_imgs)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_paths, y_encoded, test_size=0.2, random_state=22)\n",
        "\n",
        "# Convert training images to arrays\n",
        "X_train_arrs = np.array([img_to_array(load_img(image_path, target_size=(224, 224))) for image_path in X_train])\n",
        "\n",
        "# Flatten the arrays\n",
        "X_train_flattened = X_train_arrs.reshape(X_train_arrs.shape[0], -1)\n",
        "\n",
        "# Define the parameter grid for SVM\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],  # Kernel coeff for 'rbf' kernel\n",
        "    'kernel': ['rbf']  # Kernel type\n",
        "}\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_classif = SVC()\n",
        "\n",
        "# Perform grid search to find the best parameters\n",
        "svm_grid_search = GridSearchCV(svm_classif, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "svm_grid_search.fit(X_train_flattened, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = svm_grid_search.best_params_\n",
        "\n",
        "# Train the SVM classifier with the best parameters\n",
        "best_svm_classif = SVC(**best_params)\n",
        "best_svm_classif.fit(X_train_flattened, y_train)\n",
        "\n",
        "# Convert test images to arrays\n",
        "X_test_arrs = np.array([img_to_array(load_img(image_path, target_size=(224, 224))) for image_path in X_test])\n",
        "\n",
        "# Flatten the arrays\n",
        "X_test_flattened = X_test_arrs.reshape(X_test_arrs.shape[0], -1)\n",
        "\n",
        "# Test the classifier\n",
        "y_pred = best_svm_classif.predict(X_test_flattened)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accu = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Print the metrics of the model where it achieves the best accuracy\n",
        "print(\"\\nMetrics for SVM with best parameters:\")\n",
        "print(\"Accuracy:\", accu)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1:\", f1)\n",
        "\n",
        "# Store metrics in a dataframe\n",
        "metrics_df = pd.DataFrame({'Accuracy': [accu],\n",
        "                           'Precision': [prec],\n",
        "                           'Recall': [recall],\n",
        "                           'F1 Score': [f1]})\n",
        "\n",
        "# Save metrics to file\n",
        "metrics_file = \"SVM_metrics.csv\"\n",
        "metrics_df.to_csv(metrics_file, index=False)\n",
        "\n",
        "print(f\"Metrics for SVM saved to {metrics_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "JItCwrb3PUQv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOpKKY2TisLe",
        "outputId": "8f19f940-b20a-4cab-a504-30c03a16a734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Best accuracy so far: 0.7500: 100%|██████████| 144/144 [01:56<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "K-Nearest Neighbors with GridSearchCV:\n",
            "Best parameters: {'algorithm': 'auto', 'leaf_size': 20, 'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}\n",
            "Best accuracy: 0.75\n",
            "Accuracy: 0.75\n",
            "Precision: 0.875\n",
            "Recall: 0.75\n",
            "F1: 0.75\n",
            "Metrics for KNN saved to KNN_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize KNN classifier\n",
        "knn_classi = KNeighborsClassifier()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_paths, X_test, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Augment the training data\n",
        "X_train_augmented = []\n",
        "y_train_augmented = []\n",
        "for i, image_path in enumerate(X_train_paths):\n",
        "    img = load_and_preprocess_image(image_path)\n",
        "    if img is not None:\n",
        "        X_train_augmented.append(img)\n",
        "        y_train_augmented.append(y_train[i])\n",
        "        for j in range(4):  # Augment each image 4 times\n",
        "            augmented_img = datagen.random_transform(img)\n",
        "            X_train_augmented.append(augmented_img)\n",
        "            y_train_augmented.append(y_train[i])\n",
        "\n",
        "X_train_augmented = np.array(X_train_augmented)\n",
        "y_train_augmented = np.array(y_train_augmented)\n",
        "\n",
        "\n",
        "# Get all parameter combinations\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7],  # Number of neighbors to use\n",
        "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n",
        "    'leaf_size': [20, 30, 40],  # Leaf size passed to BallTree or KDTree\n",
        "    'p': [1, 2]  # Power parameter for the Minkowski metric\n",
        "}\n",
        "\n",
        "param_combinations = list(ParameterGrid(param_grid))\n",
        "\n",
        "# Variables to store best parameters and accuracy\n",
        "best_params = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Initialize tqdm with total number of iterations\n",
        "with tqdm(total=len(param_combinations)) as pbar:\n",
        "    # Perform grid search over all parameter combinations\n",
        "    for params in param_combinations:\n",
        "        # Set parameters for KNN classifier\n",
        "        knn_classi.set_params(**params)\n",
        "\n",
        "        # Convert X_test to numeric arrays and reshape\n",
        "        X_test_numeric = np.array([load_and_preprocess_image(image_path) for image_path in X_test])\n",
        "        X_test_reshaped = X_test_numeric.reshape(X_test_numeric.shape[0], -1)\n",
        "\n",
        "        # Fit KNN classifier\n",
        "        knn_classi.fit(X_train_augmented.reshape(X_train_augmented.shape[0], -1), y_train_augmented)\n",
        "\n",
        "        # Test the classifier\n",
        "        y_pred = knn_classi.predict(X_test_reshaped)\n",
        "\n",
        "        # Compute accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Update best parameters and accuracy if better\n",
        "        if accuracy > best_accuracy:\n",
        "            best_params = params\n",
        "            best_accuracy = accuracy\n",
        "\n",
        "        # Update tqdm progress bar\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(f\"Best accuracy so far: {best_accuracy:.4f}\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nK-Nearest Neighbors with GridSearchCV:\")\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best accuracy:\", best_accuracy)\n",
        "\n",
        "# Train the KNN classifier with the best parameters\n",
        "best_knn_classifier = KNeighborsClassifier(**best_params)\n",
        "best_knn_classifier.fit(X_train_augmented.reshape(X_train_augmented.shape[0], -1), y_train_augmented)\n",
        "\n",
        "# Test the classifier\n",
        "y_pred = best_knn_classifier.predict(X_test_reshaped)\n",
        "\n",
        "# Calculate metrics for KNN\n",
        "accuracy_knn = accuracy_score(y_test, y_pred)\n",
        "precision_knn = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall_knn = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1_knn = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Print additional results for KNN\n",
        "print(\"Accuracy:\", accuracy_knn)\n",
        "print(\"Precision:\", precision_knn)\n",
        "print(\"Recall:\", recall_knn)\n",
        "print(\"F1:\", f1_knn)\n",
        "\n",
        "# Store metrics in a dataframe\n",
        "metrics_df = pd.DataFrame({'Accuracy': [accuracy_knn],\n",
        "                           'Precision': [precision_knn],\n",
        "                           'Recall': [recall_knn],\n",
        "                           'F1 Score': [f1_knn]})\n",
        "\n",
        "# Save metrics to file\n",
        "metrics_file = \"KNN_metrics.csv\"\n",
        "metrics_df.to_csv(metrics_file, index=False)\n",
        "\n",
        "print(f\"Metrics for KNN saved to {metrics_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "d71ApNe8lT1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import ParameterGrid, train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def load_data(folder_path):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png', '.gif')):  # Add more image extensions if needed\n",
        "                    image_path = os.path.join(root, file)\n",
        "                    image_paths.append(image_path)\n",
        "                    labels.append(os.path.basename(os.path.dirname(image_path)))  # Use parent folder name as label\n",
        "    except FileNotFoundError:\n",
        "        print(\"Folder not found. Please check the folder path.\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred while loading the data:\", e)\n",
        "    return image_paths, labels\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error loading image: {image_path}\")\n",
        "        return None\n",
        "    resized_image = cv2.resize(image, (224, 224))  # Resize to 224x224\n",
        "    return resized_image\n",
        "\n",
        "# Load images and labels\n",
        "folder_path = '/content/drive/MyDrive/Dataset'\n",
        "image_paths, labels = load_data(folder_path)\n",
        "\n",
        "# Initialize Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_paths, X_test, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=33)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Augment the training data\n",
        "X_train_augmented = []\n",
        "y_train_augmented = []\n",
        "for i, image_path in enumerate(X_train_paths):\n",
        "    img = load_and_preprocess_image(image_path)\n",
        "    if img is not None:\n",
        "        X_train_augmented.append(img)\n",
        "        y_train_augmented.append(y_train[i])\n",
        "        for j in range(4):  # Augment each image 4 times\n",
        "            augmented_img = datagen.random_transform(img)\n",
        "            X_train_augmented.append(augmented_img)\n",
        "            y_train_augmented.append(y_train[i])\n",
        "\n",
        "X_train_augmented = np.array(X_train_augmented)\n",
        "y_train_augmented = np.array(y_train_augmented)\n",
        "\n",
        "# Convert X_test to numeric arrays and reshape\n",
        "X_test_numeric = np.array([load_and_preprocess_image(image_path) for image_path in X_test])\n",
        "X_test_reshaped = X_test_numeric.reshape(X_test_numeric.shape[0], -1)\n",
        "\n",
        "# Get all parameter combinations\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],  # Split criterion\n",
        "    'max_depth': [None, 5, 10, 15],  # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "param_combinations = list(ParameterGrid(param_grid))\n",
        "\n",
        "# Variables to store best parameters and accuracy\n",
        "best_params = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Perform grid search over all parameter combinations\n",
        "for params in param_combinations:\n",
        "    # Set parameters for Decision Tree classifier\n",
        "    dt_classifier.set_params(**params)\n",
        "\n",
        "    # Fit Decision Tree classifier\n",
        "    dt_classifier.fit(X_train_augmented.reshape(X_train_augmented.shape[0], -1), y_train_augmented)\n",
        "\n",
        "    # Test the classifier\n",
        "    y_pred = dt_classifier.predict(X_test_reshaped)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Update best parameters and accuracy if better\n",
        "    if accuracy > best_accuracy:\n",
        "        best_params = params\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "\n",
        "# Train the Decision Tree classifier with the best parameters\n",
        "best_dt_classifier = DecisionTreeClassifier(**best_params, random_state=12)\n",
        "best_dt_classifier.fit(X_train_augmented.reshape(X_train_augmented.shape[0], -1), y_train_augmented)\n",
        "\n",
        "# Test the classifier\n",
        "y_pred = best_dt_classifier.predict(X_test_reshaped)\n",
        "\n",
        "# Calculate metrics for Decision Tree\n",
        "accuracy_dt = accuracy_score(y_test, y_pred)\n",
        "precision_dt = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall_dt = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1_dt = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Print metrics for Decision Tree\n",
        "print(\"Accuracy:\", accuracy_dt)\n",
        "print(\"Precision:\", precision_dt)\n",
        "print(\"Recall:\", recall_dt)\n",
        "print(\"F1:\", f1_dt)\n",
        "\n",
        "# Store metrics in a dataframe\n",
        "metrics_df_dt = pd.DataFrame({'Accuracy': [accuracy_dt],\n",
        "                              'Precision': [precision_dt],\n",
        "                              'Recall': [recall_dt],\n",
        "                              'F1 Score': [f1_dt]})\n",
        "\n",
        "# Save metrics to file\n",
        "metrics_file_dt = \"DecisionTree_metrics.csv\"\n",
        "metrics_df_dt.to_csv(metrics_file_dt, index=False)\n",
        "\n",
        "print(f\"Metrics for Decision Tree saved to {metrics_file_dt}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVu0T44jlWfk",
        "outputId": "9f099494-93e7-42e1-ca02-118c954a7390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.375\n",
            "Precision: 0.65625\n",
            "Recall: 0.375\n",
            "F1: 0.47727272727272724\n",
            "Metrics for Decision Tree saved to DecisionTree_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50"
      ],
      "metadata": {
        "id": "QvlLgkDOXqne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the paths to your data folders\n",
        "base_dir = '/content/drive/MyDrive/Dataset'\n",
        "train_oily_dir = os.path.join(base_dir, 'oily')\n",
        "train_dry_dir = os.path.join(base_dir, 'dry')\n",
        "train_normal_dir = os.path.join(base_dir, 'normal')\n",
        "\n",
        "# Collect paths to all images in each class folder\n",
        "oily_images = [os.path.join(train_oily_dir, img) for img in os.listdir(train_oily_dir)]\n",
        "dry_images = [os.path.join(train_dry_dir, img) for img in os.listdir(train_dry_dir)]\n",
        "normal_images = [os.path.join(train_normal_dir, img) for img in os.listdir(train_normal_dir)]\n",
        "\n",
        "# Concatenate all paths and assign labels\n",
        "X_paths = oily_images + dry_images + normal_images\n",
        "y = ['oily'] * len(oily_images) + ['dry'] * len(dry_images) + ['normal'] * len(normal_images)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Preprocess images and convert them to arrays\n",
        "X_images = [load_img(image_path, target_size=(224, 224)) for image_path in X_paths]\n",
        "X_arrays = [img_to_array(img) for img in X_images]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_arrays = np.array(X_arrays)\n",
        "\n",
        "# Data augmentation for training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Load pre-trained ResNet50 model (without the top layers)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers for classification\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation='relu')(x)  # Experiment with the number of units and activation\n",
        "x = Dropout(0.5)(x)  # Experiment with the dropout rate\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Number of units equals the number of classes, softmax activation\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_arrays, y_encoded, test_size=0.2, random_state=22)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=32),\n",
        "    steps_per_epoch=len(X_train) // 32,\n",
        "    epochs=10,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_val)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_val, y_pred_labels)\n",
        "precision = precision_score(y_val, y_pred_labels, average='weighted')\n",
        "recall = recall_score(y_val, y_pred_labels, average='weighted')\n",
        "f1 = f1_score(y_val, y_pred_labels, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Store metrics in a dataframe\n",
        "metrics_df = pd.DataFrame({'Accuracy': [accuracy],\n",
        "                           'Precision': [precision],\n",
        "                           'Recall': [recall],\n",
        "                           'F1 Score': [f1]})\n",
        "\n",
        "# Save metrics to file\n",
        "metrics_file = \"ResNet50_metrics.csv\"\n",
        "metrics_df.to_csv(metrics_file, index=False)\n",
        "\n",
        "print(f\"Metrics for ResNet50 saved to {metrics_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBEL6kwmdXo3",
        "outputId": "0753bafa-8cfd-48b9-8142-56ecabeaa116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 19s 19s/step - loss: 1.1654 - accuracy: 0.4375 - val_loss: 15.5942 - val_accuracy: 0.6250\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 10s 10s/step - loss: 15.1003 - accuracy: 0.8750 - val_loss: 21.9037 - val_accuracy: 0.6250\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 10s 10s/step - loss: 19.3333 - accuracy: 0.8750 - val_loss: 24.1262 - val_accuracy: 0.6250\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 19.2049 - accuracy: 0.8750 - val_loss: 24.0291 - val_accuracy: 0.6250\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 10s 10s/step - loss: 17.0424 - accuracy: 0.8750 - val_loss: 22.3870 - val_accuracy: 0.6250\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 11.8764 - accuracy: 0.8750 - val_loss: 20.0516 - val_accuracy: 0.6250\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 13s 13s/step - loss: 21.7325 - accuracy: 0.4062 - val_loss: 21.4928 - val_accuracy: 0.6250\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 13.0215 - accuracy: 0.6562 - val_loss: 23.2445 - val_accuracy: 0.6250\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 12s 12s/step - loss: 13.0318 - accuracy: 0.8750 - val_loss: 23.9233 - val_accuracy: 0.6250\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 14.9251 - accuracy: 0.8750 - val_loss: 23.6793 - val_accuracy: 0.6250\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Accuracy: 0.625\n",
            "Precision: 0.390625\n",
            "Recall: 0.625\n",
            "F1 Score: 0.4807692307692308\n",
            "Metrics for ResNet50 saved to ResNet50_metrics.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16"
      ],
      "metadata": {
        "id": "T2LsukcfjNk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the paths to your data folders\n",
        "base_dir = '/content/drive/MyDrive/Dataset'\n",
        "train_oily_dir = os.path.join(base_dir, 'oily')\n",
        "train_dry_dir = os.path.join(base_dir, 'dry')\n",
        "train_normal_dir = os.path.join(base_dir, 'normal')\n",
        "\n",
        "# Collect paths to all images in each class folder\n",
        "oily_images = [os.path.join(train_oily_dir, img) for img in os.listdir(train_oily_dir)]\n",
        "dry_images = [os.path.join(train_dry_dir, img) for img in os.listdir(train_dry_dir)]\n",
        "normal_images = [os.path.join(train_normal_dir, img) for img in os.listdir(train_normal_dir)]\n",
        "\n",
        "# Concatenate all paths and assign labels\n",
        "X_paths = oily_images + dry_images + normal_images\n",
        "y = ['oily'] * len(oily_images) + ['dry'] * len(dry_images) + ['normal'] * len(normal_images)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Preprocess images and convert them to arrays\n",
        "X_images = [load_img(image_path, target_size=(224, 224)) for image_path in X_paths]\n",
        "X_arrays = [img_to_array(img) for img in X_images]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_arrays = np.array(X_arrays)\n",
        "\n",
        "# Data augmentation for training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Load pre-trained VGG16 model (without the top layers)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers for classification\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation='relu')(x)  # Experiment with the number of units and activation\n",
        "x = Dropout(0.5)(x)  # Experiment with the dropout rate\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Number of units equals the number of classes, softmax activation\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_arrays, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=32),\n",
        "    steps_per_epoch=len(X_train) // 32,\n",
        "    epochs=10,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_val)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_val, y_pred_labels)\n",
        "precision = precision_score(y_val, y_pred_labels, average='weighted')\n",
        "recall = recall_score(y_val, y_pred_labels, average='weighted')\n",
        "f1 = f1_score(y_val, y_pred_labels, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Store metrics in a dataframe\n",
        "metrics_df = pd.DataFrame({'Accuracy': [accuracy],\n",
        "                           'Precision': [precision],\n",
        "                           'Recall': [recall],\n",
        "                           'F1 Score': [f1]})\n",
        "\n",
        "# Save metrics to file\n",
        "metrics_file = \"VGG16_metrics.csv\"\n",
        "metrics_df.to_csv(metrics_file, index=False)\n",
        "\n",
        "print(f\"Metrics for VGG16 saved to {metrics_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb1sdHMwe7Y_",
        "outputId": "a99fbb9f-0a9f-4600-f776-e26e0d9ddb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 24s 24s/step - loss: 0.8145 - accuracy: 0.7188 - val_loss: 15.1887 - val_accuracy: 0.8750\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 23s 23s/step - loss: 5.4875 - accuracy: 0.8125 - val_loss: 10.0708 - val_accuracy: 0.8750\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 22s 22s/step - loss: 2.8351 - accuracy: 0.8125 - val_loss: 39.9529 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 23s 23s/step - loss: 10.6548 - accuracy: 0.1250 - val_loss: 2.2094 - val_accuracy: 0.8750\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 22s 22s/step - loss: 1.2000 - accuracy: 0.5625 - val_loss: 5.3349 - val_accuracy: 0.8750\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 27s 27s/step - loss: 2.6827 - accuracy: 0.7812 - val_loss: 6.6608 - val_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 27s 27s/step - loss: 3.8902 - accuracy: 0.8125 - val_loss: 6.1988 - val_accuracy: 0.8750\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 22s 22s/step - loss: 4.1501 - accuracy: 0.8125 - val_loss: 4.1736 - val_accuracy: 0.8750\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 27s 27s/step - loss: 4.0773 - accuracy: 0.7500 - val_loss: 1.7798 - val_accuracy: 0.8750\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 22s 22s/step - loss: 4.9180 - accuracy: 0.5938 - val_loss: 2.8537 - val_accuracy: 0.8750\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "Accuracy: 0.875\n",
            "Precision: 0.765625\n",
            "Recall: 0.875\n",
            "F1 Score: 0.8166666666666667\n",
            "Metrics for VGG16 saved to VGG16_metrics.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNetB0"
      ],
      "metadata": {
        "id": "wxKr1mrTieqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the paths to your data folders\n",
        "base_dir = '/content/drive/MyDrive/Dataset'\n",
        "train_oily_dir = os.path.join(base_dir, 'oily')\n",
        "train_dry_dir = os.path.join(base_dir, 'dry')\n",
        "train_normal_dir = os.path.join(base_dir, 'normal')\n",
        "\n",
        "# Collect paths to all images in each class folder\n",
        "oily_images = [os.path.join(train_oily_dir, img) for img in os.listdir(train_oily_dir)]\n",
        "dry_images = [os.path.join(train_dry_dir, img) for img in os.listdir(train_dry_dir)]\n",
        "normal_images = [os.path.join(train_normal_dir, img) for img in os.listdir(train_normal_dir)]\n",
        "\n",
        "# Concatenate all paths and assign labels\n",
        "X_paths = oily_images + dry_images + normal_images\n",
        "y = ['oily'] * len(oily_images) + ['dry'] * len(dry_images) + ['normal'] * len(normal_images)\n",
        "\n",
        "# Convert labels to numerical encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Preprocess images and convert them to arrays\n",
        "X_images = [load_img(image_path, target_size=(224, 224)) for image_path in X_paths]\n",
        "X_arrays = [img_to_array(img) for img in X_images]\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "X_arrays = np.array(X_arrays)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_arrays, y_encoded, test_size=0.2, random_state=5)\n",
        "\n",
        "# Data augmentation for training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Load pre-trained EfficientNetB0 model (without the top layers)\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers for classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)  # Experiment with the number of units and activation\n",
        "x = Dropout(0.5)(x)  # Experiment with the dropout rate\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Number of units equals the number of classes, softmax activation\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=32),\n",
        "    steps_per_epoch=len(X_train) // 32,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "precision = precision_score(y_test, y_pred_labels, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_labels, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_labels, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Store metrics in a dataframe\n",
        "metrics_df = pd.DataFrame({'Accuracy': [accuracy],\n",
        "                           'Precision': [precision],\n",
        "                           'Recall': [recall],\n",
        "                           'F1 Score': [f1]})\n",
        "\n",
        "# Save metrics to file\n",
        "metrics_file = \"EfficientNetB0_metrics.csv\"\n",
        "metrics_df.to_csv(metrics_file, index=False)\n",
        "\n",
        "print(f\"Metrics for EffiNetB0 saved to {metrics_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR11ZiQKijsN",
        "outputId": "401bbfbe-d874-447b-f359-f8783b22e048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 42s 42s/step - loss: 1.2494 - accuracy: 0.1562 - val_loss: 1.2813 - val_accuracy: 0.2500\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 16s 16s/step - loss: 0.5882 - accuracy: 0.8438 - val_loss: 0.9588 - val_accuracy: 0.3750\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.6224 - accuracy: 0.8125 - val_loss: 0.8247 - val_accuracy: 0.7500\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 12s 12s/step - loss: 0.4150 - accuracy: 0.8125 - val_loss: 0.8532 - val_accuracy: 0.7500\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.4352 - accuracy: 0.8750 - val_loss: 0.8542 - val_accuracy: 0.7500\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.4732 - accuracy: 0.8750 - val_loss: 0.9619 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.1411 - accuracy: 0.9375 - val_loss: 1.0840 - val_accuracy: 0.3750\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.2166 - accuracy: 0.8750 - val_loss: 1.1258 - val_accuracy: 0.3750\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.4142 - accuracy: 0.8125 - val_loss: 1.1545 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 12s 12s/step - loss: 0.2142 - accuracy: 0.9375 - val_loss: 1.1137 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7cab743cd750> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "Accuracy: 0.5\n",
            "Precision: 0.90625\n",
            "Recall: 0.5\n",
            "F1 Score: 0.5750000000000001\n",
            "Metrics for EffiNetB0 saved to EfficientNetB0_metrics.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing to find the best accurate model"
      ],
      "metadata": {
        "id": "XFkHm0IYjRyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# List of model names\n",
        "model_names = [\"SVM\", \"KNN\", \"DecisionTree\", \"VGG16\", \"ResNet50\", \"EfficientNetB0\"]\n",
        "\n",
        "# Load metrics for each model into a DataFrame\n",
        "metrics_data = {}\n",
        "for model_name in model_names:\n",
        "    metrics_file = f\"{model_name}_metrics.csv\"\n",
        "    metrics_data[model_name] = pd.read_csv(metrics_file)\n",
        "\n",
        "# Get accuracy for each model\n",
        "accuracies = {}\n",
        "for model_name, metrics_df in metrics_data.items():\n",
        "    accuracies[model_name] = metrics_df['Accuracy'].iloc[0]\n",
        "\n",
        "# Determine the best accurate model\n",
        "best_model = max(accuracies, key=accuracies.get)\n",
        "\n",
        "# Print the best accurate model\n",
        "print(\"Best accurate model based on the accuracy:\", best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqvDElbEjUfE",
        "outputId": "1ed5eb62-d74a-4484-e330-c236cb3b2339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accurate model based on the accuracy: VGG16\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}